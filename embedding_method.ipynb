{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "import importlib\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import xml.etree.ElementTree as ET\n",
    "import GraphModelContrastivev5\n",
    "importlib.reload(GraphModelContrastivev5)\n",
    "from GraphModelContrastivev5 import SemanticFilter\n",
    "from MethodInfo import MethodInfo\n",
    "from MutantInfo import MutantInfo\n",
    "import ChangeImpactDataBuilder\n",
    "from ChangeImpactDataBuilder import ChangeImpactDataBuilder\n",
    "from ChangeImpactMutantIndicesDataset import ChangeImpactMutantIndicesDataset\n",
    "from ChangeImpactNodeIndicesDataset import ChangeImpactNodeIndicesDataset\n",
    "\n",
    "import config\n",
    "importlib.reload(config)\n",
    "from config import *\n",
    "import random\n",
    "from util import my_collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device=\"cuda\"\n",
    "debug=False\n",
    "mutant_batch_size_train=400\n",
    "node_batch_size_train=7000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "method_info=MethodInfo()\n",
    "mutant_info=MutantInfo(method_info,debug=debug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indices=np.load(model_state_home+f\"{path_sep}train_indices.npy\").tolist()\n",
    "\n",
    "if debug:\n",
    "    train_indices=[i for i in range(800)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_builder=ChangeImpactDataBuilder(method_info,mutant_info)\n",
    "\n",
    "node_dataset=ChangeImpactNodeIndicesDataset(method_info)\n",
    "node_loader_train=DataLoader(node_dataset,batch_size=node_batch_size_train,shuffle=True,collate_fn=my_collate_fn)\n",
    "\n",
    "mutant_dataset_train=ChangeImpactMutantIndicesDataset(train_indices)\n",
    "mutant_loader_train=DataLoader(mutant_dataset_train,batch_size=mutant_batch_size_train,shuffle=True,collate_fn=my_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_methods_from_csv(file_dir:str):\n",
    "    methods_df=pd.read_csv(file_dir,encoding=\"utf-8\")\n",
    "    names=methods_df[\"method name\"].tolist()\n",
    "    bodies=methods_df[\"method body\"].tolist()\n",
    "    methods=[]\n",
    "    for i in range(len(names)):\n",
    "        methods.append(names[i]+str(bodies[i]))\n",
    "    return methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=AutoTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
    "model=AutoModel.from_pretrained(\"microsoft/codebert-base\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_data(raw_data):\n",
    "    embeddings = []\n",
    "    # max_length = tokenizer.model_max_length\n",
    "    for text in raw_data:\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            last_hidden_state = outputs.last_hidden_state\n",
    "            cls_embedding = last_hidden_state[:, 0, :]\n",
    "            embeddings.append(cls_embedding.to(\"cpu\"))\n",
    "\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data=get_methods_from_csv(method_dir)\n",
    "# raw_data=get_methods_from_callgraph(callgraph_dir)\n",
    "print(type(raw_data))\n",
    "print(len(raw_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitline_raw=[]\n",
    "splitline_idx=[]\n",
    "start=0\n",
    "end=0\n",
    "for method in raw_data:\n",
    "    splitline=method.split(\"\\n\")\n",
    "    start=end\n",
    "    end=start+len(splitline)\n",
    "    splitline_raw+=splitline\n",
    "    splitline_idx.append([start,end])\n",
    "print(len(splitline_raw))\n",
    "print(len(splitline_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitline_idx=torch.tensor(splitline_idx)\n",
    "print(splitline_idx.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_embeddings=embed_data(raw_data).squeeze(dim=1)\n",
    "all_embeddings=embed_data(splitline_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_embed_list=[]\n",
    "for i in range(splitline_idx.shape[0]):\n",
    "    e = torch.stack(all_embeddings[splitline_idx[i][0].item():splitline_idx[i][1].item()]).squeeze(1)\n",
    "    e.to(device)\n",
    "    all_embed_list.append(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "se_model=SemanticFilter(all_embeddings,splitline_idx)\n",
    "se_model.initialize_model()\n",
    "optimizer=optim.Adam(se_model.parameters(),lr=0.001)\n",
    "se_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_embeddings=[]\n",
    "n_method=splitline_idx.shape[0]\n",
    "\n",
    "for epoch in range(40):\n",
    "    if (epoch%5)==0:\n",
    "        with torch.no_grad():\n",
    "            cur_merge_embeddings=se_model.merge(all_embed_list)\n",
    "            merge_embeddings.append(cur_merge_embeddings.cpu().numpy())\n",
    "    epoch_loss=0\n",
    "    for mutant_indexs in tqdm(mutant_loader_train):\n",
    "        for node_indexs in node_loader_train:\n",
    "            change_embeddings,node_embeddings,edge_indexs,node_predict_indexs,node_predict_labels,node_predict_types,node_change_indexs,node_mutant_predict_indexs,node_predict_indexs_origin,st_embeddings=data_builder.build_batch_data(mutant_indexs,node_indexs)\n",
    "            change_embeddings=change_embeddings.to(device)\n",
    "            st_embeddings=st_embeddings.to(device)\n",
    "            node_embeddings=node_embeddings.to(device)\n",
    "            edge_indexs=edge_indexs.to(device)\n",
    "\n",
    "            info_loss=se_model.forward([all_embed_list[i.item()] for i in node_predict_indexs],device,node_predict_indexs,node_predict_labels,node_predict_types,node_change_indexs)\n",
    "            epoch_loss+=info_loss.item()\n",
    "            optimizer.zero_grad()\n",
    "            info_loss.backward()\n",
    "            optimizer.step()\n",
    "            print(f\"epoch {epoch} loss: {info_loss.item()}\\n\")\n",
    "\n",
    "# for epoch in range(120):\n",
    "#     if (epoch%20)==0:\n",
    "#         with torch.no_grad():\n",
    "#             cur_merge_embeddings=se_model.merge(all_embed_list)\n",
    "#             merge_embeddings.append(cur_merge_embeddings.cpu().numpy())\n",
    "#     start=0\n",
    "#     end=0\n",
    "#     batch_size=1000\n",
    "#     pick_list=list(range(n_method))\n",
    "#     random.shuffle(pick_list)\n",
    "#     epoch_loss=0\n",
    "#     while end < n_method:\n",
    "#         end+=batch_size\n",
    "#         info_loss=se_model.forward(pick_list[start:end],device=device)\n",
    "#         epoch_loss+=info_loss.item()\n",
    "#         start=end\n",
    "#         optimizer.zero_grad()\n",
    "#         info_loss.backward()\n",
    "#         optimizer.step()\n",
    "#     print(f\"epoch {epoch} loss: {info_loss.item()}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge_embeddings=[]\n",
    "# for split in splitline_idx:\n",
    "#     start=split[0]\n",
    "#     end=split[1]\n",
    "#     line_embeddings=all_embeddings[start:end]\n",
    "#     merge_embedding=torch.sum(line_embeddings,dim=0)/(end-start)\n",
    "#     merge_embeddings.append(merge_embedding)\n",
    "# merge_embeddings=torch.stack(merge_embeddings)\n",
    "# print(merge_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_embeddings=all_embeddings.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(merge_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 3, figsize=(20, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, embeddings in enumerate(merge_embeddings):\n",
    "    pca = PCA(n_components=2)\n",
    "    reduced = pca.fit_transform(embeddings)\n",
    "\n",
    "    ax = axes[i]\n",
    "    ax.scatter(reduced[:, 0], reduced[:, 1], s=1)\n",
    "    ax.set_title(f\"Matrix {i+1}\")\n",
    "    ax.set_xlabel(\"PC 1\")\n",
    "    ax.set_ylabel(\"PC 2\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(splitline_idx,method_splitlins_dir)\n",
    "# torch.save(merge_embeddings,method_embedding_dir)\n",
    "torch.save(merge_embeddings[0],method_embedding_dir)\n",
    "# torch.save(all_embeddings,method_embedding_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
